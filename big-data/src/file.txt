大数据:分而治之，并行计算，计算向数据移动，数据本地化读取

Hadoop --解决海量数据存储和计算分析问题 包含三种核心组件
HDFS--分布式文件存储系统 解决海量数据存储问题
yarn--集群资源管理和任务调度框架 解决资源任务调度问题
mapreduce 分布式计算框架 海量计算问题

hdfs特点:
适合存储大规模的数据 适合一次写入 多次读取
文件按照字节切分多个block块 每个块默认128M
每个block块默认有三个副本 提高容错性 如果一个块丢失 后续可以自动恢复
适合大文件写入 不适合大量小文件写入
多线程只能有一个线程写入 不能同时写
写入后不支持修改 只能追加

hdfs--文件系统  主从架构
 文件线性按照字节切割成块block 默认128M 查找block块寻找大概10ms 具有offset id
 文件于文件的block大小可以不一致---感觉有问题，和下一条冲突 只是最后一块可以不一致
 一个文件除最后一个block，其他大小一致---最后一个可能没有这么多了
 block的大小依据硬件的I/O调整 IO每秒读取速度决定大小
 block 被分散在集群的节点中 具有location---知道每个block分布到那个节点上
 block具有副本，副本没有主从概念  副本不能出现在同一节点上
 副本是满足可靠性和性能的关键
 文件上传可以指定block的大小和副本数 ，上传后只能修改副本数
 一次写入多次读取，不支持修改---修改后会改变副本的大小--会造成后面副本偏移量不准确，修正副本大小会造成集群
大量节点IO，代价太大 当前和后续副本重新处理大小
 支持追加数据

架构模型是主从架构--主节点不存储block只记录存到那个从节点
由一个nameNode(主)和dataNode(从)组成
面向文件: 文件数据(data)和文件元数据(metadata)
nameNode存储和管理元数据 并维护一个目录树  由两个组件管理 Editlog和FsImage
dataNode存储文件数据(block) 并负责读写 和nameNode维持心跳

nameNode是基于内存存储数据 元数据 目录树 block关系映射 持久化(日志+快照)  副本防止策略
dataNode 基于本地磁盘存储block 并保持block的校验和可靠性 于nameNode保持心跳 汇报块信息
启动时汇报 默认6小时汇报一次

HDFS搭建是会格式化  将会产生一个空的FsImage
当nameNode启动时 会从硬盘读完Editlog和FsImage 并将editlog作用大内存的FsImage上 然后写会硬盘的FsImage
删除editlog 因为已经持久化到FsImage上了
Editlog--集群启动之后所有的指令都会记录到这里 保证数据不丢失
FsImage 存储当前的HDFS中文件属性(文件名称、路径、权限关系。副本数。访问时间等等) 当HDFS启动后
首先将磁盘的FsImage加载到内存中 FsImage不存储block所在的datanode的信息 这些信息是由HDFS启动后
datanode周期通过心跳向namenode报告

nameNode启动时会进入安全模式的特殊状态
处于安全模式的NameNode不会进行数据的复制
nameNode从所有的datanode接受心跳和块信息
块的数量达到最小值 将会认为是安全的 在一定百分比-可配置 的块是安全的后 等30s将会退出安全模式
之后没有达到指定块数的副本 将会进行复制到其他datanoede上

为什么由安全模式: 分布式数据一致性最重要
namenode保存元数据(文件属性，块的位置) 块的位置信息是不会被持久化的 文件属性会
因为假设某台datanode启动失败然后namenode返回去 就会造成文件下载失败
所引会由安全模式 等待datanode汇报块的信息


SND--secondaryNameNode
在非Ha(一个namenode)下，SND是独立的节点周期的完成editlog和FsImage合并
默认3600s一次 editlog默认64MB(没到3600秒文件到达64MB也会合并) 操作一百万次也会(一分钟检查一次)

clent  和namenode 和datanode交互
负责切片 上传

安全模式--元数据只读操作 当集群启动时 会进入安全模式
namenode会加载FsImage 和editlog
SND 会对FsImage 和editlog 合并
等datanode上传块信息 直到副本数满足最小副本条件--整个文件系统有99.9%block达到了最小副本数 默认1
30s后退出安全模式


副本放置策略 机架服务器 ---横长高扁扁的  多个机架服务器可以放在一个柜子上 不同柜子之间的机架服务器通过
交换机通信 同一个柜子的机架服务器通过一个交换机连接

第一个副本 放置在上传的datanode上面 如果集群外(客户端不在datanode这台机器上) 随机一台不太忙的
第二个副本 放置在另一个机柜的机架服务器上 --防止这个柜子的交换机坏了然后两个副本都不能使用
第三个副本 放置在第二个副本同一个机柜不同节点 --- 减少交换机之间的交互
更多其他副本随机


写策略
客户端和namenode建立连接 namenode 判断元数据是否有效 触发副本放置策略 返回有序的datanode列表
客户端和第一台建立pipeline 管道连接 第一台node 和第二台建立pipeline 第二台和第三台pipeline
客户端将块切分packet(64kb) 并使用chunk(512b)和chucksum(4b)填充 然后发给第一台
第一台收到packet本地保存 并发送给第二台(同时接受客户端传来的后续的packet)
第二台收到保存 并传给第三台 --这流式工作 相当于并行执行 大大提高了效率 传输三台服务器仅比只传输一台慢一点点
当一个block传输完成后datanode各自向namenode汇报
都完成后 客户端传输下一个块
假设中间有datanode挂了 假设第一台挂了 客户端直接和第二台建立连接 第二台会汇报接受到位置继续传输
当block传输完成 namenode收到 发现少了 会通知完成的node复制一份

读流程
为了降低带宽和读取延迟 HDFS会尽量读取最近的副本
namenode会按照副本距离最近返回读取的datanode节点
下载一个文件 就是获取所有的block节点
客户端可以根据offset自定义连接datanode进行下载 ---同时获取所有的block 并行获取 大量节约时间 我理解的
这个是计算层次的分治 并行计算的核心



非HA高可用模式缺点: 单机故障 压力大(内存受限)
单机:namenode 主备模式  主备直接的信息同步通过JN集群同步
主备竞选主节点 每个namenode都有一个ZKFC程序 这个程序会连接ZK集群 当在ZK中抢到锁的是主节点
ZKFC程序会监控NN是否死掉 当NN死掉的时候ZKFC会释放锁  然后就会向主NN发送
确认是否挂掉 真的挂了 另一个机器就会抢到锁  然后会变成主节点
假设主节点ZKFC挂了 ZK中的锁基于session的会自动释放锁 ，然后 会向主NN发送
确认是否挂掉 发现没有 就会将主NN降级会从节点 另一台机器会自动抢到锁 然后自己升级会主节点
强锁的时候有一个回调事件--所引是知道另一台是否持有锁的
没有SNN-->备机(standby)充当这个角色
如果一台主节点-----NN网络不可用，不能和外界联系之类的  另一台机器联系不上这台 他不会升级为主节点
这是服务会不可用 当另一台恢复时，会变成服务才可用 会恢复主节点

压力大(内存受限) 多个NN 共用DN  多个NN之间联邦机机制--数据分开存在不同的NN上
多个NN 上面有一个代理层 由他去处理client的请求负载到不同的NN上
多个NN之间联邦机机制--多个NN由不同的目录blockDN存储路径不同资源最大化利用 数据隔离资源隔离




























































































